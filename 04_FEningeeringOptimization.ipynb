{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb81a7f-6527-4fd3-b8ba-fb4874f012f2",
   "metadata": {},
   "source": [
    "# REAL ESTATE PRICE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43290cc4-0927-46b2-a7ee-073ad367c9eb",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa10e4-5b7d-4b89-94df-2e7d579040f0",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fbc48ac-7ef6-4c85-a484-2b1fafe077fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reread_clipboard = False # read execution order from clipboard\n",
    "rerun_embeddings = False # OpenAI call for embeddings \n",
    "rerun_autoencoder = False # autoencoder\n",
    "rerun_one_step = True # one step vectorizer\n",
    "verbose = True # Tensorflow training verbosity\n",
    "hidden_size_1 = 1024 # parameters for the autoencoder\n",
    "hidden_size_2 = 512 \n",
    "hidden_size_3 = 256\n",
    "hidden_size_4 = 128\n",
    "latent_size = 64\n",
    "batch_size = 1024\n",
    "epochs = 1500\n",
    "catboost_task_type = 'CPU'\n",
    "test_perc = 0.2\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6183f2f-cc9c-4de6-8e28-4b3000f54a19",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e11270",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from IPython.display import Image \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))\n",
    "pd.set_option('display.min_rows', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c75873f0-e565-4714-af4e-e85344586d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from random import randint\n",
    "tf.random.set_seed(random_seed)\n",
    "#https://medium.com/analytics-vidhya/autoencoders-with-tensorflow-2f0a7315d161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85c6370-5184-4056-b9ca-cf634c61a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import timeit\n",
    "import neptune\n",
    "from neptune.utils import stringify_unsupported\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d22ac5-554e-4962-a85a-0b1bdad9eb3f",
   "metadata": {},
   "source": [
    "### Some Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520f9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc(series):\n",
    "    return series.value_counts(dropna = False)\n",
    "def myanalyzeassets(seta, setb):\n",
    "    seta = set(seta)\n",
    "    setb = set(setb)\n",
    "    print(\"in first only\" + str(len(seta - setb)))\n",
    "    print(\"in second only\" + str(len(setb - seta)))\n",
    "    print(\"in both\" + str(len(seta.intersection(setb))))\n",
    "    print(\"length of first\" + str(len(seta)))\n",
    "    print(\"length of second\" + str(len(setb)))\n",
    "def myplothisttopperc(myseries, myperc = 99, figsize=(10, 3)):\n",
    "    mycol = np.array(myseries)\n",
    "    cutoff = myperc / 100\n",
    "    nr = int(len(mycol)*cutoff)\n",
    "    myplothist(mycol[np.argsort(mycol)[:nr]], 100)\n",
    "def myplothist(series, bins = 100, figsize=(10, 3)):\n",
    "    nump = np.array(series)\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.hist(nump, bins = bins)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a581213-0e41-4645-abf6-213bf4b661bf",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce617d46-e50c-4278-a15b-01da0f244b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reread_clipboard == True:\n",
    "    model_run_dictionaries = pd.read_clipboard().T\n",
    "    model_run_dictionaries = [{index: row[col] for index, row in model_run_dictionaries.iterrows()} for col in model_run_dictionaries.columns]\n",
    "    with open('../pickles/04/model_run_dictionaries.pkl', 'wb') as f:\n",
    "        pickle.dump(model_run_dictionaries, f)\n",
    "else: \n",
    "    with open('../pickles/04/model_run_dictionaries.pkl', 'rb') as f:\n",
    "        model_run_dictionaries = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1acfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_projected = pd.read_pickle('../pickles/03/selected_projected_df_from_03.pkl')\n",
    "df_aptms = pd.read_pickle('../pickles/03/aptms_df.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d141f97-bac5-4896-ab90-49253506d900",
   "metadata": {},
   "source": [
    "## OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a66bd-e373-4438-8e46-a070838449db",
   "metadata": {},
   "source": [
    "### High Level Overview - The Main Function in Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72adea4-84f4-45b0-a4d0-7bb446e3c1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"features.png\" width=\"2000\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"features.png\", width=2000) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d4a5e98-8680-4319-8f0b-33240f1f8ad3",
   "metadata": {},
   "source": [
    "The main variables:\n",
    "df_aptms: each apartment is one row, contains 11 structured features about an apartment and the asking price. \n",
    "          'the main apartment dataframe'  \n",
    "df_selected_projected: one keyword detected for a given apartment per row of the dataframe. \n",
    "df_selected_projected has 475 keywords that fall into 391 minor categories (as there are some near-repetitions), and 22 major categories (forming a two-level hierarchical structure), \n",
    "           with over 213k rows, all-in-all.\n",
    "\n",
    "def optimize(model_run_dictionaries, df_selected_projected, df_aptms):\n",
    "    # get ready\n",
    "    for # iterate over model_run_dictionaries - variable i\n",
    "        if model_run_dictionaries[i]['switched_on'] == True:\n",
    "\t\t\t# remove outliers: criteria=price/m2_price - as the model cannot detect e.g. 1m EUR apartments correctly \n",
    "\t\t\t# perform a train test split - so that there will be no train-test leakage \n",
    "            # split the global variables df_apmts and df_selected_projected to train and test\n",
    "\t\t\t# understand appearance frequencies - how many times each keyword occurs, and what is the percentage of a focal keyword within its category \n",
    "\t\t\t\t(the latter is needed for \"relative boundary\" one-hot-encoding)\n",
    "            # create a list with all dataframe to be horizontally concatenated - add the first entry: df_aptms; train and test are always separated\n",
    "            if model_run_dictionaries[i]['gpt_on'] == True:\n",
    "\t\t\t\t# add keyword based features\n",
    "            if model_run_dictionaries[i]['embeddings_on'] == True:\n",
    "\t\t\t\t# use keyword based features and turn them into embeddings\n",
    "                if model_run_dictionaries[i]['autoencoder_on'] == True:\n",
    "\t\t\t\t\t# instead of using embedding based features turn these into autoencodings \n",
    "\t\t\t# prepare dataframes X and y - concatenate, median impute, but also, for catboost, return the list of categorical features\n",
    "\t\t\t# log with Neptune.AI\n",
    "\t\t\t# run catboost\n",
    "\t\t\t# log results into model_result_dictionaries\n",
    "    return model_result_dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e806cc-5e86-40c9-a053-e8a5d39eebf3",
   "metadata": {},
   "source": [
    "### Get Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2b981d-cfbe-463e-bf16-782695d5a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_data_and_read_with_leakage(df_selected_projected): \n",
    "    \"\"\"\n",
    "    THERE IS A TRAINING-TESTING LEAKAGE HERE, AS THE APPEARANCE FREQUENCIES ARE CALCULATED ON THE WHOLE DATASET.\n",
    "    Enriches the keyword detections dataframe with e1 - e1 is minor category, e2 is keyword and Key is major category in the output. \n",
    "    Returns the all_columns variable and the appearance frequencies dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df_selected_projected (DataFrame): The keyword detections dataframe.\n",
    "    \n",
    "    Returns:\n",
    "    df_selected_projected (DataFrame): The enriched keyword detections dataframe.\n",
    "    all_columns (list): List of all columns in the appearance frequencies dataframe.\n",
    "    df_appearance_freqs (DataFrame): The keyword-minor-major-category frequency dataframe.\n",
    "    \"\"\"\n",
    "    df_appearance_freqs = pd.read_pickle('../pickles/otherPickles/appearance_freqs_df.pkl')\n",
    "    all_columns = sorted(df_appearance_freqs.feature_e1.tolist())\n",
    "    df_selected_projected = df_selected_projected.merge(df_appearance_freqs[['e2', 'e1']], how='left', on='e2')\n",
    "    df_selected_projected['feature_e1'] = df_selected_projected['Key'] + '---' + df_selected_projected['e1']\n",
    "    return df_selected_projected, all_columns, df_appearance_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413280ba-274b-4083-b52a-4cae1a74ee45",
   "metadata": {},
   "source": [
    "### Filter and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d42413-51f8-4a56-8fa6-4d293df5c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_split_to_train_test(df_aptms, df_selected_projected, model_run_dictionary, plot_param = 99.9):\n",
    "    \"\"\"\n",
    "    Filters and splits the data into train and test sets based on the given criteria put forth in the current model_run_dictionary.\n",
    "\n",
    "    Args:\n",
    "        df_aptms (DataFrame): The input DataFrame containing the main apartment dataframe.\n",
    "        df_selected_projected (DataFrame): The input DataFrame containing the keyword detections dataframe.\n",
    "        model_run_dictionary (dict): A dictionary containing model run parameters.\n",
    "        plot_param (float, optional): The parameter for plotting. Defaults to 99.9.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the filtered DataFrame (df_aptms - the main apartment dataframe), \n",
    "        the filtered DataFrame (df_selected_projected - the keyword detections dataframe),\n",
    "               the indices of the train set (train_idx), and the indices of the test set (test_idx).\n",
    "    \"\"\"\n",
    "    if model_run_dictionary['model_split_criteria'] == 'price':\n",
    "        df_aptms = df_aptms[df_aptms.price < model_run_dictionary['model_price_limit']]\n",
    "        myplothisttopperc(df_aptms.price, plot_param)\n",
    "    elif model_run_dictionary['model_split_criteria'] == 'm2_price':\n",
    "        ds_m2_price = df_aptms.price / df_aptms.Uldpind\n",
    "        df_aptms = df_aptms[ds_m2_price < model_run_dictionary['model_price_limit']]\n",
    "        myplothisttopperc(df_aptms.price / df_aptms.Uldpind, plot_param)\n",
    "    else: \n",
    "        raise Exception(f\"unhandled model split criteria: {model_run_dictionary['model_split_criteria']}\")\n",
    "        \n",
    "    df_selected_projected = df_selected_projected[df_selected_projected.idx.isin(df_aptms.index.tolist())]\n",
    "    \n",
    "    train_test = df_aptms.index.tolist()\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(train_test)\n",
    "    test_idx = train_test[:int(test_perc*len(train_test))]\n",
    "    train_idx = train_test[int(test_perc*len(train_test)):]\n",
    "    return df_aptms, df_selected_projected, train_idx, test_idx\n",
    "\n",
    "def apply_tt_split(df_aptms, df_selected_projected, train_idx, test_idx):\n",
    "    \"\"\"\n",
    "    Splits the given dataframes into train and test sets based on the provided indices.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_aptms (DataFrame): The dataframe containing apartment dataframe.\n",
    "    - df_selected_projected (DataFrame): The dataframe containing the keyword detections dataframe.\n",
    "    - train_idx (list): The list of indices to be used for the train set.\n",
    "    - test_idx (list): The list of indices to be used for the test set.\n",
    "    \n",
    "    Returns:\n",
    "    - df_selected_projected_train (DataFrame): The train set of the keyword detections dataframe.\n",
    "    - df_aptms_train (DataFrame): The train set of the apartment dataframe.\n",
    "    - df_selected_projected_test (DataFrame): The test set of the keyword detections dataframe.\n",
    "    - df_aptms_test (DataFrame): The test set of the apartment dataframe.\n",
    "    \"\"\"\n",
    "    df_selected_projected_train = df_selected_projected[df_selected_projected.idx.isin(train_idx)].copy()\n",
    "    df_selected_projected_test = df_selected_projected[df_selected_projected.idx.isin(test_idx)].copy()\n",
    "    \n",
    "    df_aptms_train = df_aptms[df_aptms.index.isin(train_idx)].copy()\n",
    "    df_aptms_test = df_aptms[df_aptms.index.isin(test_idx)].copy()\n",
    "    \n",
    "    return df_selected_projected_train, df_aptms_train, df_selected_projected_test, df_aptms_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536f88f-d302-4d39-84f8-67c709738a5c",
   "metadata": {},
   "source": [
    "### Understand Appearance Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39275a4-74e7-4c98-96bb-7ecc67754217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_previous_data_relative_absolute_frequencies_without_leakage_after_split_not_turned_on(df_selected_projected): \n",
    "    ### create dense-sparse separation: dense will one-hot encode the more prevalent features, sparse will use two categorical columns, losing only a little bit of the remaninig information\n",
    "    ## (e.g. 5k out of 208k, in the first trial example settings set)\n",
    "    ## df_appearance_freqs shows the relative frequency of each minor category within each major category\n",
    "    #df_appearance_freqs = df_selected_projected.groupby(['Key', 'e2']).count().reset_index(drop = False).sort_values(['Key', 'idx'], ascending = [True, False])\n",
    "    #df_appearance_freqs = df_appearance_freqs.merge(df_appearance_freqs.groupby(['Key'])[['idx']].sum().reset_index().rename(columns = {'idx' : 'idx2'}), how = 'left', on = 'Key')\n",
    "    #df_appearance_freqs.idx2 = df_appearance_freqs.idx / df_appearance_freqs.idx2\n",
    "    #\n",
    "    #df_appearance_freqs = df_appearance_freqs.merge(pd.read_pickle('../pickles/otherPickles/appearance_df.pkl'), on = ['Key', 'e2'], how = 'left')\n",
    "    #df_appearance_freqs['feature'] = df_appearance_freqs['Key'] + '---' + df_appearance_freqs['e2']\n",
    "    #df_appearance_freqs['LongFeature'] = df_appearance_freqs['LongKey'] + '---' + df_appearance_freqs['e2']\n",
    "    #df_appearance_freqs['LongFeatureEN'] = df_appearance_freqs['LongKey'] + '---' + df_appearance_freqs['English']\n",
    "\n",
    "    return df_appearance_freqs, all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1819e5e-67b2-4afd-ad30-b3ce9f60e3a0",
   "metadata": {},
   "source": [
    "### Add Keyword Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad768714-7b34-4909-b323-155e1b7f8f23",
   "metadata": {},
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9ab122-cba4-4f49-8406-1a77c485081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt_dense_features(df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, model_run_dictionary, df_selected_projected_test, df_aptms_index_test):\n",
    "    def add_remaining_columns_as_zeros(all_columns, df):\n",
    "        \"\"\"\n",
    "        Adds the missing columns as zeros to the given DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - all_columns (list): List of all column names.\n",
    "        - df (pandas.DataFrame): The DataFrame to add the columns to.\n",
    "\n",
    "        Returns:\n",
    "        - pandas.DataFrame: The DataFrame with the added missing columns as zeros.\n",
    "        \"\"\"\n",
    "        added_columns = list(set(all_columns) - set(df.columns))\n",
    "        for column in added_columns:\n",
    "            df[column] = 0\n",
    "        return df\n",
    "        \n",
    "    def get_feature_lists(df_appearance_freqs, model_run_dictionary):\n",
    "        \"\"\"\n",
    "        Get feature lists based on appearance frequencies and model run dictionary.\n",
    "\n",
    "        Args:\n",
    "            df_appearance_freqs (DataFrame): keyword-minor-major-category frequency dataframe.\n",
    "            model_run_dictionary (dict): Dictionary containing model run parameters.\n",
    "\n",
    "        Returns:\n",
    "            tuple: these three small dataframes containing feature names in 'e1' column\n",
    "                - df_dense_features_one: used for one-hot-encoding based on the relative frequencies (uses global parameter model_run_dictionaries[dense_sparse_relative_boundary])\n",
    "                - df_dense_features_two: used for one-hot-encoding based on the absolute frequencies [of each minor category within the entire dataset] (in order to not double-include, it uses \n",
    "                                                                variables model_run_dictionaries[dense_sparse_relative_boundary] and model_run_dictionaries[dense_sparse_absolute_boundary])\n",
    "                - df_sparse_features: used for encoding in two categorical columns, later - the rest of the features, not included above, go there. \n",
    "        \"\"\"\n",
    "        df_dense_features_one = df_appearance_freqs[df_appearance_freqs.idx2_e1 >= model_run_dictionary['dense_sparse_relative_boundary']].copy()\n",
    "        df_dense_features_two = df_appearance_freqs[(df_appearance_freqs.idx_e1 >= model_run_dictionary['dense_sparse_absolute_boundary']) & \\\n",
    "                                                    (df_appearance_freqs.idx2_e1 < model_run_dictionary['dense_sparse_relative_boundary'])].copy()\n",
    "        df_sparse_features = df_appearance_freqs[(df_appearance_freqs.idx2_e1 < model_run_dictionary['dense_sparse_relative_boundary']) & (\\\n",
    "                                                    df_appearance_freqs.idx_e1 < model_run_dictionary['dense_sparse_absolute_boundary'])].copy()\n",
    "        return df_dense_features_one, df_dense_features_two, df_sparse_features\n",
    "    \n",
    "    def get_sources_for_wide_dfs(df_selected_projected_train, df_dense_features_one, df_dense_features_two, df_sparse_features, df_selected_projected_test):\n",
    "        \"\"\"\n",
    "        Retrieves the required sources for creating wide dataframes.\n",
    "\n",
    "        Parameters:\n",
    "        - df_selected_projected_train: keyword detections dataframe - for training\n",
    "        - df_dense_features_one: DataFrame containing dense features one\n",
    "        - df_dense_features_two: DataFrame containing dense features two\n",
    "        - df_sparse_features: DataFrame containing sparse features\n",
    "        - df_selected_projected_test: keyword detections dataframe - for testing\n",
    "\n",
    "        Returns:\n",
    "        - gpt_dense_source_one: DataFrame containing one-hot-encoded data for dense features one - long format\n",
    "        - gpt_dense_source_two: DataFrame containing one-hot-encoded data for dense features two - long format\n",
    "        - gpt_sparse_source: DataFrame containing data for creating categorical columns - long fromat\n",
    "        - gpt_dense_source_one_test: DataFrame containing one-hot-encoded test data for dense features one - long format\n",
    "        - gpt_dense_source_two_test: DataFrame containing one-hot-encoded test data for dense features two - long format\n",
    "        - gpt_sparse_source_test: DataFrame containing test data for creating categorical columns - long format\n",
    "        \"\"\"\n",
    "        # creates the two one-hot-encoded long dataframes\n",
    "        gpt_dense_source_one = df_selected_projected_train[df_selected_projected_train.e1.isin(df_dense_features_one.e1.tolist())].copy()\n",
    "        gpt_dense_source_two = df_selected_projected_train[df_selected_projected_train.e1.isin(df_dense_features_two.e1.tolist())].copy()\n",
    "\n",
    "        # creates the dataframe later used for creating categorical columns - long one\n",
    "        gpt_sparse_source = df_selected_projected_train[df_selected_projected_train.e1.isin(df_sparse_features.e1.tolist())].copy()\n",
    "\n",
    "        # same for test\n",
    "        gpt_dense_source_one_test = df_selected_projected_test[df_selected_projected_test.e1.isin(df_dense_features_one.e1.tolist())].copy()\n",
    "        gpt_dense_source_two_test = df_selected_projected_test[df_selected_projected_test.e1.isin(df_dense_features_two.e1.tolist())].copy()\n",
    "        gpt_sparse_source_test = df_selected_projected_test[df_selected_projected_test.e1.isin(df_sparse_features.e1.tolist())].copy()\n",
    "\n",
    "        return gpt_dense_source_one, gpt_dense_source_two, gpt_sparse_source, gpt_dense_source_one_test, gpt_dense_source_two_test, gpt_sparse_source_test\n",
    "    \n",
    "    def get_dense_wide_table(gpt_dense_source_one_train, gpt_dense_source_two_train, df_aptms_index_train, gpt_dense_source_one_test, gpt_dense_source_two_test, df_aptms_index_test):\n",
    "        \"\"\"\n",
    "        Generate wide versions of the dense features and create the final dense dataframe.\n",
    "\n",
    "        Parameters:\n",
    "        gpt_dense_source_one_train (DataFrame): The first dense source train dataframe - long format\n",
    "        gpt_dense_source_two_train (DataFrame): The second dense source train dataframe - long format\n",
    "        df_aptms_index_train (DataFrame): The train index dataframe from the main apartement dataframe.\n",
    "        gpt_dense_source_one_test (DataFrame): The first dense source test dataframe - long format.\n",
    "        gpt_dense_source_two_test (DataFrame): The second dense source test dataframe - long format.\n",
    "        df_aptms_index_test (DataFrame): The test index dataframe from the main apartement dataframe.\n",
    "\n",
    "        Returns:\n",
    "        gpt_dense_source_final_train (DataFrame): The final dense train dataframe.\n",
    "        gpt_dense_source_final_test (DataFrame): The final dense test dataframe.\n",
    "        \"\"\"\n",
    "        ## work with the dense part of the data\n",
    "        # create proper feature names - containing both the major and the minor category\n",
    "        gpt_dense_source_one_train['feature_e1'] = gpt_dense_source_one_train['Key'] + '---' + gpt_dense_source_one_train['e1']\n",
    "        gpt_dense_source_two_train['feature_e1'] = gpt_dense_source_two_train['Key'] + '---' + gpt_dense_source_two_train['e1']\n",
    "        \n",
    "        # create two dataframes with wide versions of the dense features\n",
    "        gpt_dense_source_one_wide = gpt_dense_source_one_train.pivot_table(index='idx', columns='feature_e1', values='e1', aggfunc='count').fillna(0)\n",
    "        gpt_dense_source_one_wide_np = (np.array(gpt_dense_source_one_wide) != 0) * 1 # after introducing e1, this table was no longer binary, making it binary now\n",
    "        gpt_dense_source_one_wide = pd.DataFrame(gpt_dense_source_one_wide_np, index=gpt_dense_source_one_wide.index, columns=gpt_dense_source_one_wide.columns)\n",
    "        gpt_dense_source_two_wide = gpt_dense_source_two_train.pivot_table(index='idx', columns='feature_e1', values='e1', aggfunc='count').fillna(0)\n",
    "        gpt_dense_source_two_wide_np = (np.array(gpt_dense_source_two_wide) != 0) * 1 # after introducing e1, this table was no longer binary, making it binary now\n",
    "        gpt_dense_source_two_wide = pd.DataFrame(gpt_dense_source_two_wide_np, index=gpt_dense_source_two_wide.index, columns=gpt_dense_source_two_wide.columns)\n",
    "\n",
    "        ## create the final dense dataframe and sort its columns\n",
    "        gpt_dense_source_final_train = pd.DataFrame(df_aptms_index_train, columns=['idx']).merge(gpt_dense_source_one_wide, how='left', on='idx').merge(gpt_dense_source_two_wide, how='left', on='idx').set_index('idx').fillna(0)\n",
    "        gpt_dense_source_final_train = gpt_dense_source_final_train[sorted(gpt_dense_source_final_train.columns)]\n",
    "\n",
    "        # same for test + adding missing columns\n",
    "        gpt_dense_source_one_test['feature_e1'] = gpt_dense_source_one_test['Key'] + '---' + gpt_dense_source_one_test['e1']\n",
    "        gpt_dense_source_two_test['feature_e1'] = gpt_dense_source_two_test['Key'] + '---' + gpt_dense_source_two_test['e1']\n",
    "\n",
    "        gpt_dense_source_one_wide_test = gpt_dense_source_one_test.pivot_table(index='idx', columns='feature_e1', values='e1', aggfunc='count').fillna(0)\n",
    "        gpt_dense_source_one_wide_np_test = (np.array(gpt_dense_source_one_wide_test) != 0) * 1 # after introducing e1, this table was no longer binary, making it binary now\n",
    "        gpt_dense_source_one_wide_test = pd.DataFrame(gpt_dense_source_one_wide_np_test, index=gpt_dense_source_one_wide_test.index, columns=gpt_dense_source_one_wide_test.columns)\n",
    "        gpt_dense_source_two_wide_test = gpt_dense_source_two_test.pivot_table(index='idx', columns='feature_e1', values='e1', aggfunc='count').fillna(0)\n",
    "        gpt_dense_source_two_wide_np_test = (np.array(gpt_dense_source_two_wide_test) != 0) * 1 # after introducing e1, this table was no longer binary, making it binary now\n",
    "        gpt_dense_source_two_wide_test = pd.DataFrame(gpt_dense_source_two_wide_np_test, index=gpt_dense_source_two_wide_test.index, columns=gpt_dense_source_two_wide_test.columns)\n",
    "\n",
    "        gpt_dense_source_final_test = pd.DataFrame(df_aptms_index_test, columns=['idx']).merge(gpt_dense_source_one_wide_test, how='left', on='idx').merge(gpt_dense_source_two_wide_test, how='left', on='idx').set_index('idx').fillna(0)\n",
    "\n",
    "        # what columns to add\n",
    "        all_dense_columns = sorted(gpt_dense_source_final_train.columns.tolist())\n",
    "        # adding columns\n",
    "        gpt_dense_source_final_test = add_remaining_columns_as_zeros(all_dense_columns, gpt_dense_source_final_test)\n",
    "        gpt_dense_source_final_test = gpt_dense_source_final_test[sorted(gpt_dense_source_final_test.columns)]\n",
    "\n",
    "        return gpt_dense_source_final_train, gpt_dense_source_final_test\n",
    "\n",
    "    def dense_train_sanity_check(df_selected_projected, df_dense_features_one, df_dense_features_two, gpt_dense_source_final):\n",
    "        \"\"\"\n",
    "        Performs a sanity check on the dense train data.\n",
    "\n",
    "        Parameters:\n",
    "        - df_selected_projected: keyword detections dataframe\n",
    "        - df_dense_features_one: DataFrame containing the first set of dense features\n",
    "        - df_dense_features_two: DataFrame containing the second set of dense features\n",
    "        - gpt_dense_source_final: Dense matrix for the train data\n",
    "\n",
    "        Returns:\n",
    "        - d_all_dense: Total number of cells in the dense train matrix\n",
    "        - d_full_dense: Number of non-zero cells in the dense train matrix\n",
    "        \"\"\"\n",
    "        ## sanity check - are all the main occurencies ok, for dense train\n",
    "        print(f'*The number of train data to be encoded: {df_selected_projected[df_selected_projected.e1.isin(pd.concat([df_dense_features_one.e1, df_dense_features_two.e1], axis = 0).tolist())].shape[0]}')\n",
    "        print(f'*The numer of train non-zeroes in the encoded matrix: {(np.array(gpt_dense_source_final) != 0).sum()}')\n",
    "        print('The two numbers may differ, as some detections used keyword synonyms for the same e1 minor features. DEBUGNB! Assertion itself has thus has been removed.')\n",
    "        #assert df_selected_projected[df_selected_projected.e1.isin(pd.concat([df_dense_features_one.e1, df_dense_features_two.e1], axis = 0).tolist())].shape[0] == \\\n",
    "        #                                                                                                                (np.array(gpt_dense_source_final) != 0).sum()\n",
    "        #print(\"All correctly cast to dense matrix half\")\n",
    "        d_all_dense = gpt_dense_source_final.shape[0]*gpt_dense_source_final.shape[1]\n",
    "        print(f'*Dense train all cells: {d_all_dense}')\n",
    "        d_full_dense = (np.array(gpt_dense_source_final) != 0).sum()\n",
    "        print(f'*Dense train cells full: {d_full_dense}')\n",
    "        print(f'*Percent full train, thus: {d_full_dense/d_all_dense}')\n",
    "\n",
    "        return d_all_dense, d_full_dense\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs the GPT dense features pipeline.\n",
    "\n",
    "    Args:\n",
    "        df_selected_projected_train (DataFrame): keyword detections train dataframe\n",
    "        df_appearance_freqs (DataFrame): keyword-minor-major-category frequency dataframe.\n",
    "        df_aptms_index_train (DataFrame): main train apartment dataframe indices. \n",
    "        model_run_dictionary (dict): The dictionary containing model run information.\n",
    "        df_selected_projected_test (DataFrame): keyword detections test dataframe.\n",
    "        df_aptms_index_test (DataFrame): The test dataframe containing apartment indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - gpt_dense_source_final_train (DataFrame): The final dense source dataframe.\n",
    "            - gpt_sparse_source_train (DataFrame): The sparse source dataframe.\n",
    "            - d_all_dense (dict): The dictionary containing all dense features.\n",
    "            - d_full_dense (dict): The dictionary containing full dense features.\n",
    "            - gpt_dense_source_final_test (DataFrame): The final dense source test dataframe.\n",
    "            - gpt_sparse_source_test (DataFrame): The sparse source test dataframe.\n",
    "    \"\"\"\n",
    "    df_dense_features_one, df_dense_features_two, df_sparse_features = get_feature_lists(df_appearance_freqs_train, model_run_dictionary)\n",
    "    gpt_dense_source_one_train, gpt_dense_source_two_train, gpt_sparse_source_train, gpt_dense_source_one_test, gpt_dense_source_two_test, gpt_sparse_source_test = get_sources_for_wide_dfs( \\\n",
    "                                                                        df_selected_projected_train, df_dense_features_one, df_dense_features_two, df_sparse_features, df_selected_projected_test)\n",
    "    gpt_dense_source_final_train, gpt_dense_source_final_test = get_dense_wide_table(gpt_dense_source_one_train, gpt_dense_source_two_train, df_aptms_index_train, gpt_dense_source_one_test, gpt_dense_source_two_test, df_aptms_index_test)\n",
    "    \n",
    "    d_all_dense, d_full_dense = dense_train_sanity_check(df_selected_projected_train, df_dense_features_one, df_dense_features_two, gpt_dense_source_final_train)\n",
    "\n",
    "    return gpt_dense_source_final_train, gpt_sparse_source_train, d_all_dense, d_full_dense, gpt_dense_source_final_test, gpt_sparse_source_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ccdc4-5b8b-4604-adb3-6e99adbbe7c3",
   "metadata": {},
   "source": [
    "#### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fd567b-09b1-4464-8fd5-bc97c3457e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt_sparse_features(df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, gpt_sparse_source_train, d_full_dense, \\\n",
    "                            df_selected_projected_test, df_aptms_index_test, gpt_sparse_source_test):\n",
    "    def enrich_sparse_source_with_frequencies(gpt_sparse_source, df_appearance_freqs):\n",
    "        \"\"\"\n",
    "        Enriches the given sparse source with appearance frequencies.\n",
    "\n",
    "        Parameters:\n",
    "        gpt_sparse_source (DataFrame): The sparse source DataFrame.\n",
    "        df_appearance_freqs (DataFrame): The DataFrame containing appearance frequencies.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: The enriched sparse source DataFrame.\n",
    "        \"\"\"\n",
    "        gpt_sparse_source = gpt_sparse_source.merge(df_appearance_freqs[['e1', 'idx_e1']].rename(columns = {'idx_e1' : 'appearance_cnt'}), how = 'left', on = 'e1') \\\n",
    "                                                                                    .sort_values(['idx', 'Key', 'appearance_cnt'], ascending = [True, True, False])\n",
    "        return gpt_sparse_source\n",
    "    \n",
    "    def get_sparse_tuples(gpt_sparse_source):   \n",
    "        \"\"\"\n",
    "        Generate sparse tuples from the gpt_sparse_source DataFrame.\n",
    "        !!! - comment this some more, about how only top 2 are taken\n",
    "        Args:\n",
    "            gpt_sparse_source (DataFrame): The input dataframe containing the data.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A dataframe with sparse tuples, separated into two columns.\n",
    "        \"\"\"\n",
    "        # create sparse_tuples - a dataframe with a tuple for each apartment-major category combination; the tuple contains the top 2 from right above\n",
    "        gpt_sparse_source = gpt_sparse_source.sort_values(['idx', 'Key', 'appearance_cnt'], ascending=[True, True, False]) # sorting will make sure that the top 2 are the most prevalent, just in case sorting again\n",
    "        \n",
    "        def tere(myframe):\n",
    "            if myframe.shape[0] == 0:\n",
    "                return (pd.NA, pd.NA)\n",
    "            if myframe.shape[0] == 1:\n",
    "                return (myframe.e1.iloc[0], pd.NA)\n",
    "            return (myframe.e1.iloc[0], myframe.e1.iloc[1])\n",
    "        \n",
    "        sparse_tuples = gpt_sparse_source.groupby(['idx', 'Key']).apply(tere).reset_index().rename(columns={0: 'tuple'}) \n",
    "\n",
    "        # for sparse_tuples, separate the tuple to two new columns\n",
    "        def tere(mytuple):\n",
    "            return mytuple[0]\n",
    "        \n",
    "        def tere2(mytuple):\n",
    "            return mytuple[1]\n",
    "        \n",
    "        sparse_tuples['tuple1'] = sparse_tuples.tuple.apply(tere)\n",
    "        sparse_tuples['tuple2'] = sparse_tuples.tuple.apply(tere2)\n",
    "        \n",
    "        # in order to make the sparse_tuples into a long vector, create two new columns for later aligning with sparse matrix column names\n",
    "        sparse_tuples['Key1'] = 'sparse_' + sparse_tuples['Key'] + '_1'\n",
    "        sparse_tuples['Key2'] = 'sparse_' + sparse_tuples['Key'] + '_2'\n",
    "\n",
    "        return sparse_tuples\n",
    "    \n",
    "    def get_sparse_wide_table(sparse_tuples, df_aptms_index):\n",
    "        \"\"\"\n",
    "        First takes sparse tuples in two columns and creates a single column from them - renaming column names in the process. \n",
    "        Converts sparse tuples into a wide table format.\n",
    "\n",
    "        Args:\n",
    "            sparse_tuples (pd.DataFrame): DataFrame containing sparse tuples.\n",
    "            df_aptms_index (pd.DataFrame): DataFrame containing the neccessary rows from the main apartment dataframe.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Wide table format of the sparse tuples.\n",
    "        \"\"\"\n",
    "        # Make the sparse_tuples into sparse_tuples_long. Since the second value sometimes contains NA in the wide format, disregard these.\n",
    "        sparse_tuples_long = pd.concat([sparse_tuples[['idx', 'Key1', 'tuple1']], sparse_tuples[['idx', 'Key2', 'tuple2']].rename(columns={'Key2': 'Key1', 'tuple2': 'tuple1'})], axis=0).rename(columns={'Key1': 'Key', 'tuple1': 'tuple'})\n",
    "        sparse_tuples_long = sparse_tuples_long[~sparse_tuples_long.tuple.isna()].sort_values('idx').reset_index(drop=True)\n",
    "\n",
    "        # Make the gpt_sparse_source_final from sparse_tuples_long and make sure to get all the rows that are necessary by using df_aptms.index.\n",
    "        gpt_sparse_source_final = sparse_tuples_long.pivot_table(index='idx', columns='Key', values='tuple', aggfunc='first')\n",
    "        gpt_sparse_source_final = pd.DataFrame(df_aptms_index, columns=['idx']).merge(gpt_sparse_source_final, how='left', on='idx').set_index('idx')\n",
    "\n",
    "        return gpt_sparse_source_final\n",
    "    \n",
    "    def get_sparse_wide_table(sparse_tuples, df_aptms_index):\n",
    "        \"\"\"\n",
    "        Converts sparse tuples into a wide table format.\n",
    "\n",
    "        Parameters:\n",
    "        sparse_tuples (DataFrame): DataFrame containing sparse tuples.\n",
    "        df_aptms_index (DataFrame): DataFrame containing the necessary rows from the main apartment dataframe.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: Wide table format of the sparse tuples.\n",
    "        \"\"\"\n",
    "        #make the sparse_tuples into sparse_tuples_long. since the second value sometimes contained NA in the wide format, disregard these\n",
    "        sparse_tuples_long = pd.concat([sparse_tuples[['idx', 'Key1', 'tuple1']], sparse_tuples[['idx', 'Key2', 'tuple2']].rename(columns = {'Key2': 'Key1', 'tuple2': 'tuple1'})], axis = 0). \\\n",
    "                            rename(columns = {'Key1': 'Key', 'tuple1': 'tuple'})\n",
    "        sparse_tuples_long = sparse_tuples_long[~sparse_tuples_long.tuple.isna()].sort_values('idx').reset_index(drop = True)\n",
    "\n",
    "        #make the gpt_sparse_source_final from sparse_tuples_long and make sure to get all the rows that are necessary by using df_aptms.index\n",
    "        gpt_sparse_source_final = sparse_tuples_long.pivot_table(index = 'idx', columns = 'Key', values = 'tuple', aggfunc = 'first')\n",
    "        gpt_sparse_source_final = pd.DataFrame(df_aptms_index, columns = ['idx']).merge(gpt_sparse_source_final, how = 'left', on = 'idx').set_index('idx')\n",
    "\n",
    "        return gpt_sparse_source_final\n",
    "    \n",
    "    def sparse_train_sanity_check(gpt_sparse_source_final, d_full_dense, df_selected_projected):\n",
    "        \"\"\"\n",
    "        Perform a sanity check on the sparse training data.\n",
    "\n",
    "        Parameters:\n",
    "        - gpt_sparse_source_final (DataFrame): The final sparse source data.\n",
    "        - d_full_dense (int): The number of cells in the dense training data.\n",
    "        - df_selected_projected (DataFrame): The selected projected data.\n",
    "\n",
    "        Returns:\n",
    "        - d_all_sparse (int): The total number of cells in the sparse training data.\n",
    "        - d_full_sparse (int): The number of non-null cells in the sparse training data.\n",
    "        \"\"\"\n",
    "        ## sanity check - are all the main occurencies ok, for sparse\n",
    "        print('*Not performing the sparse sanity check, as only top 2 made the cut')\n",
    "        d_all_sparse = gpt_sparse_source_final.shape[0]*gpt_sparse_source_final.shape[1]\n",
    "        print(f'*Sparse train all cells: {d_all_sparse}')\n",
    "        d_full_sparse = d_all_sparse - gpt_sparse_source_final.isna().sum().sum()\n",
    "        print(f'*Sparse train cells full: {d_full_sparse}')\n",
    "        print(f'*Percent full train, thus: {d_full_sparse/d_all_sparse}')\n",
    "        d_loss_percent = 1 - (d_full_dense + d_full_sparse)/df_selected_projected.shape[0]\n",
    "        print(f'*Training losses: {d_loss_percent}, which is {df_selected_projected.shape[0] - d_full_dense - d_full_sparse} out of {df_selected_projected.shape[0]}')\n",
    "        return d_all_sparse, d_full_sparse\n",
    "\n",
    "    \"\"\"\n",
    "    Runs the GPT sparse features pipeline on the given data.\n",
    "\n",
    "    Args:\n",
    "        df_selected_projected_train (DataFrame): keyword detections train dataframe\n",
    "        df_appearance_freqs_train (DataFrame): keyword-minor-major-category frequency dataframe.\n",
    "        df_aptms_index_train (DataFrame): the main train apartment dataframe indices.\n",
    "        gpt_sparse_source_train (DataFrame): The GPT sparse source train data.\n",
    "        d_full_dense (dict): The full dense data.\n",
    "        df_selected_projected_test (DataFrame): keyword detections test dataframe.\n",
    "        df_aptms_index_test (DataFrame): the main test apartment dataframe indices.\n",
    "        gpt_sparse_source_test (DataFrame): The GPT sparse source test data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the final GPT sparse source data for train and test respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## work with the sparse part of the data\n",
    "    # within each apartment-major category combination, sort by relative occurrence of minor categories - this will next allow to select only the top 2\n",
    "    if gpt_sparse_source_train.empty == False:\n",
    "        # for train - with the sanity check\n",
    "        gpt_sparse_source_train = enrich_sparse_source_with_frequencies(gpt_sparse_source_train, df_appearance_freqs_train)\n",
    "        sparse_tuples = get_sparse_tuples(gpt_sparse_source_train)\n",
    "        gpt_sparse_source_final = get_sparse_wide_table(sparse_tuples, df_aptms_index_train)\n",
    "        d_all_sparse, d_full_sparse = sparse_train_sanity_check(gpt_sparse_source_final, d_full_dense, df_selected_projected_train) # the final assembly step is after sanity check, as otherwise \"NA\"s hinder\n",
    "\n",
    "        # same for test\n",
    "        gpt_sparse_source_test = enrich_sparse_source_with_frequencies(gpt_sparse_source_test, df_appearance_freqs_train) # use appearance freqs from train!\n",
    "        sparse_tuples_test = get_sparse_tuples(gpt_sparse_source_test)\n",
    "        gpt_sparse_source_final_test = get_sparse_wide_table(sparse_tuples_test, df_aptms_index_test)\n",
    "    else:\n",
    "        gpt_sparse_source_final = pd.DataFrame([], columns = ['idx', 'e1'])\n",
    "        gpt_sparse_source_final_test = pd.DataFrame([], columns = ['idx', 'e1'])\n",
    "\n",
    "    return gpt_sparse_source_final, gpt_sparse_source_final_test # do not return gpt_sparse_source_train, as I am just checking, whether it is empty later, and the current enriching does not add to this check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a04def5-cc3a-4adb-a893-fa2895ce4c60",
   "metadata": {},
   "source": [
    "#### Run Dense and Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9651823-1ab8-4849-89cf-53c30c41a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_and_encode(gpt_dense_source_final, gpt_sparse_source_final, there_is_sparse_part):\n",
    "    \"\"\"\n",
    "    Combines the dense and sparse matrices to create the final GPT matrix for encoding.\n",
    "\n",
    "    Parameters:\n",
    "    gpt_dense_source_final (pandas.DataFrame): The dense matrix for GPT.\n",
    "    gpt_sparse_source_final (pandas.DataFrame): The sparse matrix for GPT.\n",
    "    there_is_sparse_part (bool): Indicates if there is a sparse part in the matrix.\n",
    "\n",
    "    Returns:\n",
    "    gpt_final (pandas.DataFrame): The final GPT matrix after combining the dense and sparse matrices.\n",
    "    \"\"\"\n",
    "    print(f'there_is_sparse_part: {there_is_sparse_part}')\n",
    "    if there_is_sparse_part:\n",
    "        # make the sparse matrix into categorical - for catboost\n",
    "        gpt_sparse_source_final = gpt_sparse_source_final.fillna(\"NA\")\n",
    "        gpt_sparse_source_final = gpt_sparse_source_final.astype('category')\n",
    "\n",
    "        # assemble the final gpt4 matrix\n",
    "        gpt_final = pd.concat([gpt_dense_source_final, gpt_sparse_source_final], axis = 1)  \n",
    "    else:\n",
    "        gpt_final = gpt_dense_source_final.copy()\n",
    "    return gpt_final\n",
    "    \n",
    "def run_gpt_features(df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, model_run_dictionary, df_selected_projected_test, df_aptms_index_test):\n",
    "    \"\"\"\n",
    "    Runs the GPT features generation process.\n",
    "\n",
    "    Args:\n",
    "        df_selected_projected_train (DataFrame): keyword detections train dataframe.\n",
    "        df_appearance_freqs_train (DataFrame): keyword-minor-major-category frequency train dataframe.\n",
    "        df_aptms_index_train (DataFrame): the main apartment train dataframe indices.\n",
    "        model_run_dictionary (dict): The dictionary containing model run information.\n",
    "        df_selected_projected_test (DataFrame): The selected projected test dataframe.\n",
    "        df_aptms_index_test (DataFrame): The test dataframe containing aptms index.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the final GPT features for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    gpt_dense_source_final_train, gpt_sparse_source_train, d_all_dense, d_full_dense, gpt_dense_source_final_test, gpt_sparse_source_test = run_gpt_dense_features( \\\n",
    "                                                                df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, model_run_dictionary, df_selected_projected_test, df_aptms_index_test)\n",
    "\n",
    "    gpt_sparse_source_final_train, gpt_sparse_source_final_test = run_gpt_sparse_features(df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, gpt_sparse_source_train, d_full_dense, \\\n",
    "                                                      df_selected_projected_test, df_aptms_index_test, gpt_sparse_source_test) # d_full_dense for sanity check \n",
    "    \n",
    "    if gpt_sparse_source_train.empty == False:\n",
    "        gpt_final_train = get_final_and_encode(gpt_dense_source_final_train, gpt_sparse_source_final_train, there_is_sparse_part = True)\n",
    "        gpt_final_test = get_final_and_encode(gpt_dense_source_final_test, gpt_sparse_source_final_test, there_is_sparse_part = True)\n",
    "    else: \n",
    "        gpt_final_train = get_final_and_encode(gpt_dense_source_final_train, gpt_sparse_source_final_train, there_is_sparse_part = False)\n",
    "        gpt_final_test = get_final_and_encode(gpt_dense_source_final_test, gpt_sparse_source_final_test, there_is_sparse_part= False)\n",
    "    return gpt_final_train, gpt_final_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4029c5d-1e9b-4470-b2f4-77b2a3aca0ed",
   "metadata": {},
   "source": [
    "### Add Embedding Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7255166-0a2f-469d-ae92-91724dbddb6d",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dfae233-3dfb-43c3-82d7-b31af4e1938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_remaining_columns_as_zeros(df, all_columns):\n",
    "    \"\"\"\n",
    "    Adds the missing columns as zeros to the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to add the columns to.\n",
    "    all_columns (list): A list of all columns that should be present in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with the missing columns added as zeros.\n",
    "    \"\"\"\n",
    "    missing_columns = set(all_columns) - set(df.columns)\n",
    "    df[list(missing_columns)] = 0\n",
    "    return df[sorted(df.columns)] # sorting built in here, already!!!\n",
    "\n",
    "def load_pickle_files(filepaths):\n",
    "    \"\"\"\n",
    "    Load and return a list of objects from pickle files.\n",
    "\n",
    "    Parameters:\n",
    "    filepaths (list): A list of filepaths to the pickle files.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of objects loaded from the pickle files.\n",
    "    \"\"\"\n",
    "    return [pickle.load(open(filepath, 'rb')) for filepath in filepaths]\n",
    "\n",
    "def save_embeddings_to_pickle(embeddings_data, filepaths):\n",
    "    \"\"\"\n",
    "    Save embeddings data to pickle files.\n",
    "\n",
    "    Args:\n",
    "        embeddings_data (list): List of embeddings data.\n",
    "        filepaths (list): List of filepaths to save the embeddings data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for data, filepath in zip(embeddings_data, filepaths):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "def compute_embeddings(df, embeddings, keywords, index_df, all_columns, short_shortest, mean_max):\n",
    "    \"\"\"\n",
    "    Compute embeddings for a given dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input dataframe.\n",
    "    - embeddings (numpy.ndarray): The embeddings array.\n",
    "    - keywords (list): The list of keywords.\n",
    "    - index_df (pandas.DataFrame): The index dataframe.\n",
    "    - all_columns (list): The list of all columns.\n",
    "    - short_shortest (str): The prefix for column names. ('short' or 'shortest').\n",
    "    - mean_max (str): The method for computing embeddings ('mean' or 'max').\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The dataframe with computed embeddings.\n",
    "    \"\"\"\n",
    "    df_wide = df.pivot_table(index='idx', columns='feature_e1', values='Key', aggfunc='count').fillna(0).astype(bool).astype(int)\n",
    "    df_wide = add_remaining_columns_as_zeros(df_wide, all_columns)\n",
    "    assert df_wide.columns.tolist() == keywords\n",
    "\n",
    "    def tere(row):\n",
    "        filtered_embeddings = embeddings[row.astype(bool)]\n",
    "        return filtered_embeddings.mean(axis=0) if mean_max == 'mean' else filtered_embeddings.max(axis=0)\n",
    "    embeddings_block = np.vstack(df_wide.apply(tere, axis=1))\n",
    "    \n",
    "    col_prefix = short_shortest\n",
    "    col_prefix += \"_mean_emb_\" if mean_max == 'mean' else \"_max_emb_\"\n",
    "    embeddings_df = pd.DataFrame(embeddings_block, columns=[col_prefix + str(i) for i in range(embeddings_block.shape[1])], index=df_wide.index)\n",
    "    return pd.DataFrame(index_df, columns=['idx']).merge(embeddings_df, how='left', on='idx').set_index('idx').fillna(0)\n",
    "\n",
    "def run_embeddings_features_by_machine(df_selected_projected_train, df_appearance_freqs_train, df_aptms_index_train, df_selected_projected_test, df_aptms_index_test, all_columns):\n",
    "    \"\"\"\n",
    "    Runs the embeddings features by aggregating the embeddings data of a single apartment - there are four results, two for each of the two embeddings types \n",
    "    (short_embeddings[queries openai for major-categoryEN--minor categoryEN combination, wherein major category and thus hierarhical indices dominate] and \n",
    "    shortest_embeddings[queries openai for minor-categoryEN only, where the hierarchical information does not dominate]), \n",
    "    and two for each of the two aggregation types (mean_pooling and max_pooling), ALL COMBINATIONS OF THE 2X2. \n",
    "\n",
    "    Args:\n",
    "        df_selected_projected_train (DataFrame): keyword detections dataframe.\n",
    "        df_appearance_freqs_train (DataFrame): keyword-minor-major-category frequency dataframe, train w/ leakage\n",
    "        df_aptms_index_train (DataFrame): the main apartment dataframe indices.\n",
    "        df_selected_projected_test (DataFrame): The selected projected test DataFrame.\n",
    "        df_aptms_index_test (DataFrame): The DataFrame containing aptms index for test data.\n",
    "        all_columns (list): The list of all columns.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the embeddings features by machine, with keys representing different configurations and values representing the train and test embeddings blocks.\n",
    "    \"\"\"\n",
    "    embeddings_paths = ['../pickles/035/embeddings_zero_header_short.pkl', '../pickles/035/embeddings_zero_short.pkl',\n",
    "                        '../pickles/035/embeddings_zero_header_shortest.pkl', '../pickles/035/embeddings_zero_shortest.pkl']\n",
    "    dl_short, da_short, dl_shortest, da_shortest = load_pickle_files(embeddings_paths)\n",
    "\n",
    "    configs = [{'short_shortest': 'short', 'mean_max': 'mean', 'embeddings': da_short, 'keywords': dl_short},\n",
    "               {'short_shortest': 'shortest', 'mean_max': 'mean', 'embeddings': da_shortest, 'keywords': dl_shortest},\n",
    "               {'short_shortest': 'short', 'mean_max': 'max', 'embeddings': da_short, 'keywords': dl_short},\n",
    "               {'short_shortest': 'shortest', 'mean_max': 'max', 'embeddings': da_shortest, 'keywords': dl_shortest}]\n",
    "\n",
    "    pickle_filepaths = []\n",
    "    base_path = \"../pickles/04/embeddings_block_gpt_\"\n",
    "\n",
    "    for config in configs:\n",
    "        for dataset_type in ['train', 'test']:\n",
    "            file_name = f\"{config['short_shortest']}_{config['mean_max']}_{dataset_type}.pkl\"\n",
    "            pickle_filepaths.append(base_path + file_name)\n",
    "\n",
    "    embeddings_blocks = []\n",
    "    if rerun_embeddings:\n",
    "        for config in configs:\n",
    "            train_embeddings = compute_embeddings(df_selected_projected_train, **config, index_df=df_aptms_index_train, all_columns=all_columns)\n",
    "            test_embeddings = compute_embeddings(df_selected_projected_test, **config, index_df=df_aptms_index_test, all_columns=all_columns)\n",
    "            embeddings_blocks.extend([train_embeddings, test_embeddings])\n",
    "        save_embeddings_to_pickle(embeddings_blocks, pickle_filepaths)\n",
    "    else:\n",
    "        embeddings_blocks = load_pickle_files(pickle_filepaths)\n",
    "\n",
    "    return dict(zip(['short_mean', 'shortest_mean', 'short_max', 'shortest_max'], \n",
    "                    [{'train': train, 'test': test} for train, test in zip(embeddings_blocks[::2], embeddings_blocks[1::2])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283b8a4-3994-45a3-8375-172f5f1fe1b8",
   "metadata": {},
   "source": [
    "#### Autoencodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b9b84da-908a-4f5d-b1e0-3884a4a85875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autoencoder_features(gpt_embeddings_block):\n",
    "    def train_gpt_autoencodings(gpt_embeddings_block, gpt_embeddings_block_test):\n",
    "        \"\"\"\n",
    "        Trains a GPT autoencoder model using the given input data.\n",
    "\n",
    "        Args:\n",
    "            gpt_embeddings_block (numpy.ndarray): The input data for training the autoencoder.\n",
    "            gpt_embeddings_block_test (numpy.ndarray): The input data for testing the autoencoder.\n",
    "\n",
    "        Returns:\n",
    "            keras.Model: The trained encoder model.\n",
    "        \"\"\"\n",
    "        input_layer = layers.Input(shape = gpt_embeddings_block.shape[1])\n",
    "        hidden_1 = layers.Dense(hidden_size_1, activation = 'relu')(input_layer)\n",
    "        hidden_2 = layers.Dense(hidden_size_2, activation = 'relu')(hidden_1)\n",
    "        hidden_3 = layers.Dense(hidden_size_3, activation = 'relu')(hidden_2)\n",
    "        hidden_4 = layers.Dense(hidden_size_4, activation = 'relu')(hidden_3)\n",
    "        latent = layers.Dense(latent_size, activation = 'relu')(hidden_4)\n",
    "        encoder = Model(inputs = input_layer, outputs = latent, name = 'encoder')\n",
    "        if verbose == True:\n",
    "            print(encoder.summary())\n",
    "        \n",
    "        input_layer_decoder = layers.Input(shape = encoder.output.shape)\n",
    "        upsampled_4 = layers.Dense(hidden_size_4, activation = 'relu')(input_layer_decoder)\n",
    "        upsampled_3 = layers.Dense(hidden_size_3, activation = 'relu')(upsampled_4)\n",
    "        upsampled_2 = layers.Dense(hidden_size_2, activation = 'relu')(upsampled_3)\n",
    "        upsampled_1 = layers.Dense(hidden_size_1, activation = 'relu')(upsampled_2)\n",
    "        constructed = layers.Dense(gpt_embeddings_block.shape[1], activation = None)(upsampled_1)\n",
    "        decoder = Model(inputs = input_layer_decoder, outputs = constructed, name= 'decoder')\n",
    "        if verbose == True:\n",
    "            print(decoder.summary())\n",
    "            \n",
    "        autoencoder = Model(inputs = encoder.input, outputs = decoder(encoder.output))\n",
    "        if verbose == True: \n",
    "            print(autoencoder.summary())\n",
    "        autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-9, patience=200, verbose = 1, mode='min')\n",
    "        history = autoencoder.fit(gpt_embeddings_block, gpt_embeddings_block, epochs=epochs, batch_size=batch_size, validation_data = (gpt_embeddings_block_test, gpt_embeddings_block_test), \\\n",
    "                                                                                                                        verbose = 'auto' if verbose == True else 0, callbacks = [early_stopping])\n",
    "        return encoder\n",
    "\n",
    "    def run_gpt_autoencodings(block_name, gpt_embeddings_block, encoder):\n",
    "        \"\"\"\n",
    "        Applies the encoder to the GPT embeddings block and returns the encoded embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        - block_name (str): The name of the block.\n",
    "        - gpt_embeddings_block (pd.DataFrame): The GPT embeddings block to be encoded.\n",
    "        - encoder (tf.keras.Model): The encoder model.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: The encoded GPT embeddings block.\n",
    "        \"\"\"\n",
    "        gpt_embeddings_block = pd.DataFrame(encoder(np.array(gpt_embeddings_block)).numpy(), index = gpt_embeddings_block.index, columns = [block_name+'_emb_'+str(i) for i in range(latent_size)]) \n",
    "        return gpt_embeddings_block\n",
    "        \n",
    "    def save_autoencodings(block_name, train_block, test_block):\n",
    "        \"\"\"\n",
    "        Save the autoencodings for a given block.\n",
    "\n",
    "        Parameters:\n",
    "        - block_name (str): The name of the block.\n",
    "        - train_block (object): The autoencodings for the training data.\n",
    "        - test_block (object): The autoencodings for the test data.\n",
    "        \"\"\"\n",
    "        with open('../pickles/04/ae_embeddings_block_gpt_' + block_name +'_train.pkl', 'wb') as f:\n",
    "            pickle.dump(train_block, f)\n",
    "        with open('../pickles/04/ae_embeddings_block_gpt_' + block_name +'_test.pkl', 'wb') as f:\n",
    "            pickle.dump(test_block, f)\n",
    "            \n",
    "    \n",
    "    def read_autoencodings(block_name):\n",
    "        \"\"\"\n",
    "        Read autoencodings from pickle files.\n",
    "\n",
    "        Parameters:\n",
    "        - block_name (str): The name of the block.\n",
    "\n",
    "        Returns:\n",
    "        - train_block: The autoencodings for the training data.\n",
    "        - test_block: The autoencodings for the test data.\n",
    "        \"\"\"\n",
    "        with open('../pickles/04/ae_embeddings_block_gpt_' + block_name +'_train.pkl', 'rb') as f:\n",
    "            train_block = pickle.load(f)\n",
    "        with open('../pickles/04/ae_embeddings_block_gpt_' + block_name +'_test.pkl', 'rb') as f:\n",
    "            test_block = pickle.load(f)\n",
    "        return train_block, test_block\n",
    "\n",
    "    \"\"\"\n",
    "    Runs autoencoder features on the given GPT embeddings block.\n",
    "\n",
    "    Args:\n",
    "        gpt_embeddings_block (dict): A dictionary containing GPT embeddings block.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated GPT embeddings block with autoencoder features.\n",
    "\n",
    "    \"\"\"\n",
    "    for block_name, block_content in gpt_embeddings_block.items():\n",
    "        print(block_name)#!!! off\n",
    "        if rerun_autoencoder == True: \n",
    "            current_encoder = train_gpt_autoencodings(block_content['train'], block_content['test'])\n",
    "            block_content['train'] = run_gpt_autoencodings(block_name, block_content['train'], current_encoder)\n",
    "            block_content['test'] = run_gpt_autoencodings(block_name, block_content['test'], current_encoder)\n",
    "            save_autoencodings(block_name, block_content['train'], block_content['test'])\n",
    "        else:\n",
    "            block_content['train'], block_content['test'] = read_autoencodings(block_name)\n",
    "    \n",
    "    return gpt_embeddings_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3979625-8e01-4c93-93c3-024a5a5c3653",
   "metadata": {},
   "source": [
    "### Get One-Step Vectorizer Results - right now switched off!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9bb3a-f477-42f4-85b6-c4edf927b3f2",
   "metadata": {},
   "source": [
    "#### Embeddings and Autoencodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9385155-c219-44fd-bf65-8449feb568c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_step_features(df_selected_projected_train, df_appearance_freqs, df_selected_projected_test): \n",
    "    def read_embeddings():\n",
    "        with open('../pickles/035/embeddings_zero_header_shortest.pkl', 'rb') as f:\n",
    "            dl_all_keywords_EE = pickle.load(f)\n",
    "        with open('../pickles/035/embeddings_zero_shortest.pkl', 'rb') as f:\n",
    "            da_all_embeddings = pickle.load(f)\n",
    "        with open('../pickles/035/embeddings_zero_header_categories.pkl', 'rb') as f:\n",
    "            dl_all_major_categories_EN = pickle.load(f)\n",
    "        with open('../pickles/035/embeddings_zero_categories.pkl', 'rb') as f:\n",
    "            da_all_embeddings_major = pickle.load(f)\n",
    "        return dl_all_keywords_EE, da_all_embeddings, dl_all_major_categories_EN, da_all_embeddings_major\n",
    "\n",
    "    def read_one_step_vectorizer():\n",
    "        with open('../pickles/04/step_block_one_train.pkl', 'rb') as f:\n",
    "            one_step_block = pickle.load(f)\n",
    "        with open('../pickles/04/step_block_one_test.pkl', 'rb') as f:\n",
    "            one_step_block_test = pickle.load(f)\n",
    "        return one_step_block, one_step_block_test\n",
    "    \n",
    "    def get_one_step_embeddings(df_selected_projected, df_appearance_freqs, dl_all_keywords_EE, da_all_embeddings, dl_all_major_categories_EN, da_all_embeddings_major, is_train = True): \n",
    "        df_all_embeddings = pd.DataFrame(da_all_embeddings, columns = ['e2_emb_'+str(i) for i in range(da_all_embeddings.shape[1])], index = dl_all_keywords_EE).reset_index(names = 'feature')\n",
    "        df_all_embeddings_major = pd.DataFrame(da_all_embeddings_major, columns = ['key_emb_'+str(i) for i in range(da_all_embeddings_major.shape[1])], index = dl_all_major_categories_EN).reset_index(names = 'LongKey')\n",
    "        df_selected_projected = df_selected_projected.merge(df_appearance_freqs[['e2', 'feature']], how = 'left', on = 'e2').merge(df_appearance_freqs[['feature', 'LongKey']], how = 'left', on = 'feature')\n",
    "        one_step_block = df_selected_projected.merge(df_all_embeddings, how = 'left', on = 'feature').merge(df_all_embeddings_major, how = 'left', on = 'LongKey')\n",
    "        one_step_block = one_step_block.drop(['idx', 'e2', 'Key', 'feature', 'LongKey', 'e1', 'feature_e1'], axis = 1)\n",
    "        first_half = one_step_block[['e2_emb_'+str(i) for i in range(da_all_embeddings.shape[1])]].copy()\n",
    "        first_half.columns = ['emb_'+str(i) for i in range(da_all_embeddings.shape[1])]\n",
    "        second_half = one_step_block[['key_emb_'+str(i) for i in range(da_all_embeddings.shape[1])]].copy().sample(frac = 0.25)\n",
    "        second_half.columns = ['emb_'+str(i) for i in range(da_all_embeddings.shape[1])]\n",
    "        one_step_block = pd.concat([first_half, second_half], axis = 0).sample(frac = 1)\n",
    "        print(f'ONE STEP BLOCK: {one_step_block.shape}')\n",
    "        return one_step_block\n",
    "\n",
    "    def train_one_step_autoencodings(block, block_test):\n",
    "        input_layer = layers.Input(shape = block.shape[1])\n",
    "        hidden_1 = layers.Dense(hidden_size_1, activation = 'relu')(input_layer)\n",
    "        hidden_2 = layers.Dense(hidden_size_2, activation = 'relu')(hidden_1)\n",
    "        hidden_3 = layers.Dense(hidden_size_3, activation = 'relu')(hidden_2)\n",
    "        hidden_4 = layers.Dense(hidden_size_4, activation = 'relu')(hidden_3)\n",
    "        latent = layers.Dense(latent_size, activation = 'relu')(hidden_4)\n",
    "        encoder = Model(inputs = input_layer, outputs = latent, name = 'encoder')\n",
    "        if verbose == True:\n",
    "            print(encoder.summary())\n",
    "        \n",
    "        input_layer_decoder = layers.Input(shape = encoder.output.shape)\n",
    "        upsampled_4 = layers.Dense(hidden_size_4, activation = 'relu')(input_layer_decoder)\n",
    "        upsampled_3 = layers.Dense(hidden_size_3, activation = 'relu')(upsampled_4)\n",
    "        upsampled_2 = layers.Dense(hidden_size_2, activation = 'relu')(upsampled_3)\n",
    "        upsampled_1 = layers.Dense(hidden_size_1, activation = 'relu')(upsampled_2)\n",
    "        constructed = layers.Dense(block.shape[1], activation = None)(upsampled_1)\n",
    "        decoder = Model(inputs = input_layer_decoder, outputs = constructed, name= 'decoder')\n",
    "        if verbose == True:\n",
    "            print(decoder.summary())\n",
    "            \n",
    "        autoencoder = Model(inputs = encoder.input, outputs = decoder(encoder.output))\n",
    "        if verbose == True: \n",
    "            print(autoencoder.summary())\n",
    "        autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-9, patience=200, verbose = 1, mode='min')\n",
    "        history = autoencoder.fit(block, block, epochs=epochs, batch_size=64, validation_data = (block_test, block_test), verbose = 'auto' if verbose == True else 0, callbacks=[early_stopping])\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def run_one_step_autoencodings(block, encoder, is_train = True):\n",
    "        block = pd.DataFrame(encoder(np.array(block)).numpy(), index = block.index, columns = ['emb_'+str(i) for i in range(latent_size)]) \n",
    "        if is_train == True: \n",
    "            with open('../pickles/04/step_block_one_train.pkl', 'wb') as f:\n",
    "                pickle.dump(block, f)\n",
    "        else: \n",
    "            with open('../pickles/04/step_block_one_test.pkl', 'wb') as f:\n",
    "                pickle.dump(block, f)\n",
    "        return block\n",
    "    \n",
    "    dl_all_keywords_EE, da_all_embeddings, dl_all_major_categories_EN, da_all_embeddings_major = read_embeddings()\n",
    "    if rerun_one_step == True:\n",
    "        one_step_block_train = get_one_step_embeddings(df_selected_projected_train, df_appearance_freqs, dl_all_keywords_EE, da_all_embeddings, dl_all_major_categories_EN, da_all_embeddings_major, is_train = True)\n",
    "        one_step_block_test = get_one_step_embeddings(df_selected_projected_test, df_appearance_freqs, dl_all_keywords_EE, da_all_embeddings, dl_all_major_categories_EN, da_all_embeddings_major, is_train = False) \n",
    "        encoder = train_one_step_autoencodings(one_step_block_train, one_step_block_test)\n",
    "        one_step_block_train = run_one_step_autoencodings(one_step_block_train, encoder, is_train = True)\n",
    "        one_step_block_test = run_one_step_autoencodings(one_step_block_test, encoder, is_train = False)\n",
    "    else: \n",
    "        one_step_block_train, one_step_block_test = read_one_step_vectorizer()\n",
    "\n",
    "    return one_step_block_train, one_step_block_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c0d31-3944-4aa2-a089-e711f6d8f1ed",
   "metadata": {},
   "source": [
    "### Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd455c6-5ad8-4702-bda5-a0892f9877b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_final_df(df_list_train, df_list_test):\n",
    "    def prep_df(df_list):\n",
    "        \"\"\"\n",
    "        Concatenates a list of dataframes horizontally and reorders the columns to have 'price' as the last column.\n",
    "\n",
    "        Parameters:\n",
    "        df_list (list): A list of dataframes to be concatenated.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The concatenated dataframe with 'price' as the last column.\n",
    "        \"\"\"\n",
    "        # concat together\n",
    "        df_final = pd.concat(df_list, axis=1)\n",
    "        # make price the last column - for categorical and numeric column list extraction\n",
    "        all_col = df_final.loc[:, ~df_final.columns.isin(['price'])].columns.tolist() + ['price']\n",
    "        df_final = df_final[all_col]\n",
    "        return df_final\n",
    "    \n",
    "    def get_cat_num_info__but_price_out(df_final):\n",
    "        \"\"\"\n",
    "        Get categorical and numerical column information from a DataFrame, excluding the 'price' column.\n",
    "\n",
    "        Parameters:\n",
    "        df_final (DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        cat_cols (list): List of categorical column names.\n",
    "        cat_features_index (list): List of indices of categorical columns.\n",
    "        num_cols (list): List of numerical column names, excluding the 'price' column.\n",
    "        \"\"\"\n",
    "        cat_cols = [df_final.columns[i] for i, col_dtype in enumerate(df_final.dtypes) if col_dtype == 'category']\n",
    "        cat_features_index = [df_final.columns.tolist().index(item) for item in cat_cols if item in cat_cols]\n",
    "        num_cols = df_final.loc[:,~df_final.columns.isin(cat_cols)].columns.tolist()[:-1] # does not include price\n",
    "        return cat_cols, cat_features_index, num_cols\n",
    "    \n",
    "    def calc_median_imputation(df_final, num_cols):\n",
    "        \"\"\"\n",
    "        Calculates the median imputation for numerical columns in a DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        df_final (pandas.DataFrame): The DataFrame containing the data.\n",
    "        num_cols (list): A list of numerical column names.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of median values for each numerical column.\n",
    "        \"\"\"\n",
    "        # median inputation\n",
    "        median_list = []\n",
    "        for i in range(len(num_cols)):\n",
    "            median_list.append(np.median(df_final.loc[df_final[num_cols[i]] != 0,num_cols[i]]))\n",
    "        # NB!!! Ehitusaasta (construction year) is bimodal - the current year and the average of other years. Thus perfect median imputation would understand, if the year is 2023, or the median of the rest of the data\n",
    "        return median_list\n",
    "    \n",
    "    def impute_with_median(df_final, num_cols, medians):\n",
    "        \"\"\"\n",
    "        Imputes missing values in the specified numerical columns of a DataFrame with their respective medians.\n",
    "\n",
    "        Parameters:\n",
    "        - df_final (DataFrame): The DataFrame containing the data to be imputed.\n",
    "        - num_cols (list): A list of column names representing the numerical columns to be imputed.\n",
    "        - medians (list): A list of median values corresponding to the numerical columns.\n",
    "\n",
    "        Returns:\n",
    "        - df_final (DataFrame): The DataFrame with missing values imputed using the median values.\n",
    "        \"\"\"\n",
    "        for i in range(len(num_cols)):\n",
    "            df_final.loc[df_final[num_cols[i]] == 0, num_cols[i]] = medians[i]\n",
    "        return df_final\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses the input dataframes and prepares the final dataframes for training and testing.\n",
    "\n",
    "    Args:\n",
    "        df_list_train (list): List of dataframes for training.\n",
    "        df_list_test (list): List of dataframes for testing.\n",
    "\n",
    "    Returns:\n",
    "        df_final_train (DataFrame): Preprocessed dataframe for training.\n",
    "        df_final_test (DataFrame): Preprocessed dataframe for testing.\n",
    "        cat_features_index (list): List of categorical features indices.\n",
    "        X_train (DataFrame): Input features dataframe for training.\n",
    "        X_test (DataFrame): Input features dataframe for testing.\n",
    "        y_train (DataFrame): Target variable dataframe for training.\n",
    "        y_test (DataFrame): Target variable dataframe for testing.\n",
    "    \"\"\"\n",
    "    df_final_train = prep_df(df_list_train)\n",
    "    df_final_test = prep_df(df_list_test)\n",
    "    cat_cols, cat_features_index, num_cols = get_cat_num_info__but_price_out(df_final_train)\n",
    "    dl_medians = calc_median_imputation(df_final_train, num_cols)\n",
    "    df_final_train = impute_with_median(df_final_train, num_cols, dl_medians)\n",
    "    df_final_test = impute_with_median(df_final_test, num_cols, dl_medians)\n",
    "    X_train = df_final_train.drop(['price'], axis=1).copy()\n",
    "    X_test = df_final_test.drop(['price'], axis=1).copy()\n",
    "    y_train = df_final_train[['price']].copy()\n",
    "    y_test = df_final_test[['price']].copy()\n",
    "    return df_final_train, df_final_test, cat_features_index, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c030b-b032-47bb-9aa0-34aefd9705ec",
   "metadata": {},
   "source": [
    "### Log with Neptune AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9452c0c6-15ba-4c74-8552-1debfbd4405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_neptune(model_run_dictionary, X, X_test):\n",
    "    \"\"\"\n",
    "    Initializes a Neptune run and sets the necessary parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model_run_dictionary (dict): A dictionary containing the model run parameters.\n",
    "    - X (array-like): The final train dataset.\n",
    "    - X_test (array-like): The final test dataset.\n",
    "\n",
    "    Returns:\n",
    "    - run (neptune.Run): The initialized Neptune run.\n",
    "    \"\"\"\n",
    "    run = neptune.init_run(project='jaanbi/bnd-regression', \\\n",
    "                  api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTMzZjQ1Yi1lODI3LTRiMGQtODU4MC1lYWE0MjI1MWUxMzUifQ==', \\\n",
    "                  name = model_run_dictionary['model_run_name'])\n",
    "    print(f\"*Running model: {model_run_dictionary['model_run_name']}\")\n",
    "    print(f'*The final train dataset has: {X.shape[0]} rows and {X.shape[1]} columns')\n",
    "    print(f'*The final test dataset has: {X_test.shape[0]} rows and {X_test.shape[1]} columns')\n",
    "\n",
    "    for key in model_run_dictionary:\n",
    "        run[key] = model_run_dictionary[key]\n",
    "\n",
    "    run['dataset rows'] = X.shape[0] + X_test.shape[0]\n",
    "    run['dataset train rows'] = X.shape[0]\n",
    "    run['dataset test rows'] = X_test.shape[0]\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5970e7-87b8-4e28-87db-ccd8f0203359",
   "metadata": {},
   "source": [
    "### Run CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82391484-6b4b-4508-92fc-394921c826b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Cat_model(run, cat_features, X_train, X_test,  y_train, y_test, model_run_dictionary):\n",
    "    \"\"\"\n",
    "    Runs the CatBoostRegressor model on the given training and testing data.\n",
    "\n",
    "    Parameters:\n",
    "    - run (dict): Dictionary to store the model run results.\n",
    "    - cat_features (list): List of categorical features.\n",
    "    - X_train (array-like): Training data features.\n",
    "    - X_test (array-like): Testing data features.\n",
    "    - y_train (array-like): Training data target.\n",
    "    - y_test (array-like): Testing data target.\n",
    "    - model_run_dictionary (dict): Dictionary containing model run parameters.\n",
    "\n",
    "    Returns:\n",
    "    - run (dict): Updated dictionary with model run results.\n",
    "    - model (CatBoostRegressor): Trained CatBoostRegressor model.\n",
    "    \"\"\"\n",
    "    def metrics(run, y_pred_test):\n",
    "        score = mean_squared_error(y_test, y_pred_test)\n",
    "        run['RMSE score'].append(score)\n",
    "\n",
    "    model = cb.CatBoostRegressor(task_type = catboost_task_type, iterations = model_run_dictionary['iterations'])\n",
    "    \n",
    "    #Training session\n",
    "    start = timeit.default_timer()\n",
    "    model.fit(X_train,y_train,\n",
    "             eval_set=(X_test, y_test),\n",
    "             cat_features=cat_features,\n",
    "             plot = True,\n",
    "             plot_file = 'training_plot.html',\n",
    "             use_best_model=True,\n",
    "             verbose = False)\n",
    "    stop = timeit.default_timer()\n",
    "    run['training/plot'].upload('training_plot.html')\n",
    "    run['Training time'] = stop - start\n",
    "    run[\"training/best_score\"] = stringify_unsupported(model.get_best_score())\n",
    "    run[\"training/best_iteration\"] = stringify_unsupported(model.get_best_iteration())\n",
    "    \n",
    "    #Prediction session\n",
    "    start = timeit.default_timer()\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    stop = timeit.default_timer()\n",
    "    run['Prediction time'] = stop - start\n",
    "    \n",
    "    #Performance evaluation\n",
    "    metrics(run, y_pred_test)\n",
    "\n",
    "    return run, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b90e8b-0060-41cd-a78a-1e74ce0d9d9d",
   "metadata": {},
   "source": [
    "### Log Results for Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8986a2-c409-4b82-aa7b-794361d1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(**kwargs):\n",
    "    \"\"\"\n",
    "    This function logs the keyword arguments passed to it.\n",
    "\n",
    "    Args:\n",
    "        **kwargs: Keyword arguments to be logged.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the logged keyword arguments.\n",
    "    \"\"\"\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc6baa-1f78-4830-b2cb-557f26b54dc7",
   "metadata": {},
   "source": [
    "### Run All "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937a95e7-bfe1-4c72-a3e7-71fd6a5b9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model_run_dictionaries, df_selected_projected, df_aptms):\n",
    "    \"\"\"\n",
    "    Optimize the model by performing feature engineering and running the model for each model run dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - model_run_dictionaries (list): A list of dictionaries containing the model run configurations.\n",
    "    - df_selected_projected (DataFrame): keyword detections dataframe.\n",
    "    - df_aptms (DataFrame): the main apartment dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - model_result_dictionaries (list): A list of dictionaries containing the model results.\n",
    "    \"\"\"\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    model_result_dictionaries = []\n",
    "\n",
    "    df_selected_projected, all_columns, df_appearance_freqs_train = enrich_data_and_read_with_leakage(df_selected_projected)\n",
    "\n",
    "    for i in range(len(model_run_dictionaries)):\n",
    "\n",
    "        if model_run_dictionaries[i]['switched_on'] == True:\n",
    "            print('*****************************************')\n",
    "            print('*****************************************')\n",
    "            print('*****************************************')\n",
    "            print(f'*Model run dictionary, row: {i+1}')\n",
    "            print(model_run_dictionaries[i])\n",
    "\n",
    "            df_aptms, df_selected_projected, train_idx, test_idx = filter_and_split_to_train_test(df_aptms, df_selected_projected, model_run_dictionaries[i])\n",
    "\n",
    "            df_selected_projected_train, df_aptms_train, df_selected_projected_test, df_aptms_test = apply_tt_split(df_aptms, df_selected_projected, train_idx, test_idx)\n",
    "            \n",
    "            #df_appearance_freqs_train, all_columns = read_previous_data_relative_absolute_frequencies_with_leakage(df_selected_projected_train)\n",
    "\n",
    "            df_list_train = [df_aptms_train.drop(['description', 'google_maps'], axis = 1)]\n",
    "            df_list_test = [df_aptms_test.drop(['description', 'google_maps'], axis = 1)]\n",
    "            \n",
    "            if model_run_dictionaries[i]['gpt_on'] == True:\n",
    "                gpt_final_train, gpt_final_test = run_gpt_features(df_selected_projected_train, df_appearance_freqs_train, train_idx, model_run_dictionaries[i], df_selected_projected_test, test_idx) \n",
    "                df_list_train += [gpt_final_train]    \n",
    "                df_list_test += [gpt_final_test]    \n",
    "            \n",
    "            if model_run_dictionaries[i]['embeddings_on'] == True:\n",
    "                gpt_embeddings_block = run_embeddings_features_by_machine(df_selected_projected_train, df_appearance_freqs_train, train_idx, df_selected_projected_test, test_idx, all_columns)\n",
    "                if model_run_dictionaries[i]['autoencoder_on'] == True:\n",
    "                    gpt_embeddings_block = run_autoencoder_features(gpt_embeddings_block)\n",
    "                \n",
    "                df_list_train += [value['train'] for key, value in gpt_embeddings_block.items()]\n",
    "                df_list_test += [value['test'] for key, value in gpt_embeddings_block.items()]\n",
    "\n",
    "            #if model_run_dictionaries[i]['one_step_vectorizer_on'] == True:\n",
    "            #    one_step_block_train, one_step_block_test = run_one_step_features(df_selected_projected_train, df_appearance_freqs_train, df_selected_projected_test)\n",
    "                \n",
    "                #df_list_train += [one_step_block_train]\n",
    "                #df_list_test += [one_step_block_test]\n",
    "            df_final_train, df_final_test, cat_features_index, X_train, X_test, y_train, y_test = prep_final_df(df_list_train, df_list_test)\n",
    " \n",
    "            run = start_neptune(model_run_dictionaries[i], X_train, X_test)\n",
    "            \n",
    "            run, model_cat_cat_def = run_Cat_model(run, cat_features_index, X_train, X_test,  y_train, y_test, model_run_dictionaries[i])\n",
    "\n",
    "            model_result_dictionaries.append(log(df_final_train = df_final_train, df_final_test = df_final_test, df_list_train = df_list_train, df_list_test = df_list_test, \\\n",
    "                                                cat_features_index = cat_features_index, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, \\\n",
    "                                                model_cat_cat_def = model_cat_cat_def, df_appearance_freqs_train = df_appearance_freqs_train, i = i+1))\n",
    "    print(timeit.default_timer() - start)\n",
    "    return model_result_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5318a13-2344-461b-85e2-e6994afc7f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "*****************************************\n",
      "*****************************************\n",
      "*Model run dictionary, row: 79\n",
      "{'model_run_name': 'pr-400k-TRUE-FALSE-TRUE-0.2-480-10k', 'model_split_criteria': 'price', 'model_price_limit': 400000, 'embeddings_on': True, 'autoencoder_on': True, 'one_step_vectorizer_on': False, 'gpt_on': True, 'frequency_encoding': False, 'dense_sparse_relative_boundary': 0.2, 'dense_sparse_absolute_boundary': 480, 'iterations': 10000, 'switched_on': True}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*The number of train data to be encoded: 130984\n",
      "*The numer of train non-zeroes in the encoded matrix: 125967\n",
      "The two numbers may differ, as some detections used keyword synonyms for the same e1 minor features. DEBUGNB! Assertion itself has thus has been removed.\n",
      "*Dense train all cells: 636233\n",
      "*Dense train cells full: 125967\n",
      "*Percent full train, thus: 0.19798878712672874\n",
      "*Not performing the sparse sanity check, as only top 2 made the cut\n",
      "*Sparse train all cells: 256828\n",
      "*Sparse train cells full: 30407\n",
      "*Percent full train, thus: 0.11839441182425592\n",
      "*Training losses: 0.02853379885318108, which is 4593 out of 160967\n",
      "there_is_sparse_part: True\n",
      "there_is_sparse_part: True\n",
      "short_mean\n",
      "shortest_mean\n",
      "short_max\n",
      "shortest_max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/realestate/anaconda3/envs/bnd/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/realestate/anaconda3/envs/bnd/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_13804/4162485369.py:13: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  run = neptune.init_run(project='jaanbi/bnd-regression', \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/jaanbi/bnd-regression/e/BND-327\n",
      "*Running model: pr-400k-TRUE-FALSE-TRUE-0.2-480-10k\n",
      "*The final train dataset has: 5837 rows and 420 columns\n",
      "*The final test dataset has: 1459 rows and 420 columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b210b009b5b54501936011345db0bb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.35000357600074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/realestate/anaconda3/envs/bnd/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning:\n",
      "\n",
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "/home/realestate/anaconda3/envs/bnd/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning:\n",
      "\n",
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "/home/realestate/anaconda3/envs/bnd/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning:\n",
      "\n",
      "is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_results_dictionaries = optimize(model_run_dictionaries, df_selected_projected, df_aptms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c8248c8-476c-4f60-8dcf-2eb1565e64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/model_results_dictionaries/model_results_dictionaries_79.pkl', 'wb') as f:\n",
    "    pickle.dump(model_results_dictionaries, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2288cc3-816a-4087-8d3b-29be90a8326e",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02c94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directions to go:\n",
    "# a) xgboost\n",
    "# b) NB! DONE!!!!,\n",
    "#    keyword extraction bug # NB!!! Bug: loc_LeisureRecreat\tpikakari ja stroomi rand\t4107\t0.409880; THIS COULD POSSIBLY NOT BE 41% WITHIN THE MAJOR CATEGORY - MUST BE A BUG\n",
    "# c) feature importances\n",
    "# d) russian language text off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnd",
   "language": "python",
   "name": "bnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
